## Integrating with Amazon Rekognition: Automating Photo Evaluation for the Cloudtopia Passport Office Automation Project

![Rekognition](/assets/Module2.png)

To enhance the photo evaluation workflow, I will use the Amazon Rekognition DetectFaces API. This API allows you to analyze images stored in S3, providing detailed assessments of various facial features.

When a file is uploaded to the S3 bucket, the DetectFaces API will be triggered. The API retrieves the image from S3 and evaluates it for a variety of facial features, including gender, age, smiling, sunglasses, and more. Each feature is tagged with an evaluation result (true/false) and a confidence value ranging from 0 to 100, which conveys the confidence Rekognition has in its evaluation decision. As developers, we need to decide on a confidence threshold that will serve as our criterion for success when assessing these evaluation results.

As you proceed through the following steps, you'll see how the DetectFaces API seamlessly integrates with S3 and Lambda functions to automate and enhance the passport photo validation process.

### Step 1: Attach IAM Policy Rekognition:DetectFaces to PhotoValidationProcessor

![Rekognition Add Policy](/assets/rekognition-permissions.png)

* Under the `Function overview` go to `Configuration`
* Click `Permissions`
* Click the link under `Role name` which opens the IAM Role page

![Rekognition Add Policy](/assets/inline-again.png)

* Scroll down and click `Add permissions` then click on `Create inline policy`
* On the next page, click the `Specify permissions` dropdown and enter `Rekognition`
* Click on `Read` and select `DetectFaces`
* Click `Next`

![Detect Faces Policy](/assets/detect-faces.png)

* Name your policy in the `Policy name` box
* Click `Create policy`

![Policy Name](/assets/Rekognition-policy-name.png)

You will now see that the Rekognition permission is added back on the Lambda function page under the overview in the `Permissions` tab.

![Permissions Added Rekognition](/assets/rekognition-permission-added.png)

### Step 2: Extract filename from S3 PUT Event

* Underneath the `Function overview` click on the `Code` tab
* Input code:

```py
import json
import boto3
import uuid
import datetime

# This is an AWS Lambda function that is triggered by an Amazon S3 PUT event
# When a new object is uploaded to the specified S3 bucket, this function is executed

BUCKET_NAME = "cloudtopia-images-jun-2024"  # name of the S3 bucket that triggers this Lambda function
FACE_DETAILS_THRESHOLDS = {
    "Smile": {"desiredValue": False, "minConfidence": 90},  # facial feature detection thresholds for Amazon Rekognition
    "Sunglasses": {"desiredValue": False, "minConfidence": 90},
    "EyesOpen": {"desiredValue": True, "minConfidence": 90},
    "MouthOpen": {"desiredValue": False, "minConfidence": 90}
}

rekognition_client = boto3.client('rekognition')  # create a Rekognition client to interact with the Amazon Rekognition service

def lambda_handler(event, context):
    # This is the main entry point for the Lambda function
    # event: a dictionary containing the S3 PUT event data
    # context: a dictionary containing information about the Lambda function execution
    
    # Step 1 - Extract File Name from S3 PUT Event
    # Get the file name of the uploaded object from the S3 event data
    current_file_name = extract_file_name(event)  
    print(current_file_name)  # print the file name to the CloudWatch logs

def extract_file_name(event):
    # This function extracts the file name from the S3 event data
    # event: a dictionary containing the S3 event data
    return event["Records"][0]["s3"]["object"]["key"]  # return the file name of the uploaded object
```
* Click `Deploy`
* Create a test event by clicking the down arrow next to `Test`
* Then click `Configure test event`

![Test Event & Code](/assets/test-event.png)

* In `Event name` put S3
* Click on `Template`
* Type in S3
* Click `S3 PUT`

![S3 PUT](/assets/s3-test-event.png)

* On line 23 replace the example bucket name with your bucket name
* On line 27 replace the example bucket ARN with your bucket ARN
* On line 30 replace the key with "your-file-name.png"
* Click `Save`
* Click `Test`

![Test Event](/assets/success.png)

This shows that we are successfully extracting the file name.

### Step 3: Call Rekognition DetectFaces API

Modify the code in the code editor in Lambda:

```py
import json
import boto3
import uuid
import datetime

BUCKET_NAME = "cloudtopia-images-jun-2024"

FACE_DETAILS_THRESHOLDS = {
    "Smile": {"desiredValue": False, "minConfidence": 90},
    "Sunglasses": {"desiredValue": False, "minConfidence": 90},
    "EyesOpen": {"desiredValue": True, "minConfidence": 90},
    "MouthOpen": {"desiredValue": False, "minConfidence": 90}
}

rekognition_client = boto3.client('rekognition')

def lambda_handler(event, context):
    # Step 1 - Extract File Name from PUT Event
    current_file_name = extract_file_name(event)
    
    # Step 2 - Call Rekognition DetectFaces API
    detect_faces_response = detect_faces(current_file_name)
    print(json.dumps(detect_faces_response))

def detect_faces(file_name):
    # Call the Rekognition detect_faces API to analyze the image
    return rekognition_client.detect_faces(
        Image={
            'S3Object': {
                'Bucket': BUCKET_NAME,
                'Name': file_name
            }
        },
        Attributes=['ALL']
    )

def extract_file_name(event):
    return event["Records"][0]["s3"]["object"]["key"]
```
* Click `Deploy`
* Click `Test`

![Test Event Rekognition](/assets/test-event-2.png)

There is a max size of how much data can be shown. Here is the full output of data from the Cloudwatch logs:

```json
{
    "FaceDetails": [
        {
            "BoundingBox": {
                "Width": 0.3110760748386383,
                "Height": 0.6341437697410583,
                "Left": 0.3172023892402649,
                "Top": 0.11409683525562286
            },
            "AgeRange": {
                "Low": 24,
                "High": 32
            },
            "Smile": {
                "Value": false,
                "Confidence": 99.97425842285156
            },
            "Eyeglasses": {
                "Value": false,
                "Confidence": 100
            },
            "Sunglasses": {
                "Value": false,
                "Confidence": 100
            },
            "Gender": {
                "Value": "Male",
                "Confidence": 99.9889144897461
            },
            "Beard": {
                "Value": true,
                "Confidence": 99.9964828491211
            },
            "Mustache": {
                "Value": false,
                "Confidence": 71.83219146728516
            },
            "EyesOpen": {
                "Value": true,
                "Confidence": 96.44202423095703
            },
            "MouthOpen": {
                "Value": false,
                "Confidence": 97.71915435791016
            },
            "Emotions": [
                {
                    "Type": "CALM",
                    "Confidence": 100
                },
                {
                    "Type": "SAD",
                    "Confidence": 0.009763240814208984
                },
                {
                    "Type": "CONFUSED",
                    "Confidence": 0.0004947185516357422
                },
                {
                    "Type": "SURPRISED",
                    "Confidence": 0.000014901161193847656
                },
                {
                    "Type": "DISGUSTED",
                    "Confidence": 0.000011920928955078125
                },
                {
                    "Type": "ANGRY",
                    "Confidence": 0
                },
                {
                    "Type": "FEAR",
                    "Confidence": 0
                },
                {
                    "Type": "HAPPY",
                    "Confidence": 0
                }
            ],
            "Landmarks": [
                {
                    "Type": "eyeLeft",
                    "X": 0.4091112017631531,
                    "Y": 0.3714003264904022
                },
                {
                    "Type": "eyeRight",
                    "X": 0.5479598641395569,
                    "Y": 0.36694735288619995
                },
                {
                    "Type": "mouthLeft",
                    "X": 0.4252205491065979,
                    "Y": 0.5912889242172241
                },
                {
                    "Type": "mouthRight",
                    "X": 0.5409443974494934,
                    "Y": 0.5875861644744873
                },
                {
                    "Type": "nose",
                    "X": 0.48381465673446655,
                    "Y": 0.4896681308746338
                },
                {
                    "Type": "leftEyeBrowLeft",
                    "X": 0.35443297028541565,
                    "Y": 0.3215457797050476
                },
                {
                    "Type": "leftEyeBrowRight",
                    "X": 0.43736615777015686,
                    "Y": 0.30655142664909363
                },
                {
                    "Type": "leftEyeBrowUp",
                    "X": 0.3961386978626251,
                    "Y": 0.2958410978317261
                },
                {
                    "Type": "rightEyeBrowLeft",
                    "X": 0.5171042084693909,
                    "Y": 0.3039419949054718
                },
                {
                    "Type": "rightEyeBrowRight",
                    "X": 0.5962992310523987,
                    "Y": 0.31343403458595276
                },
                {
                    "Type": "rightEyeBrowUp",
                    "X": 0.5567367076873779,
                    "Y": 0.29052606225013733
                },
                {
                    "Type": "leftEyeLeft",
                    "X": 0.38363808393478394,
                    "Y": 0.3708478510379791
                },
                {
                    "Type": "leftEyeRight",
                    "X": 0.43633604049682617,
                    "Y": 0.37253525853157043
                },
                {
                    "Type": "leftEyeUp",
                    "X": 0.40843456983566284,
                    "Y": 0.36051875352859497
                },
                {
                    "Type": "leftEyeDown",
                    "X": 0.409643292427063,
                    "Y": 0.381078839302063
                },
                {
                    "Type": "rightEyeLeft",
                    "X": 0.520198404788971,
                    "Y": 0.3698365390300751
                },
                {
                    "Type": "rightEyeRight",
                    "X": 0.5720319747924805,
                    "Y": 0.3646824359893799
                },
                {
                    "Type": "rightEyeUp",
                    "X": 0.5479605793952942,
                    "Y": 0.35600167512893677
                },
                {
                    "Type": "rightEyeDown",
                    "X": 0.547293484210968,
                    "Y": 0.3765852451324463
                },
                {
                    "Type": "noseLeft",
                    "X": 0.455732136964798,
                    "Y": 0.5120792388916016
                },
                {
                    "Type": "noseRight",
                    "X": 0.5073928236961365,
                    "Y": 0.5103216767311096
                },
                {
                    "Type": "mouthUp",
                    "X": 0.4829539656639099,
                    "Y": 0.5634878873825073
                },
                {
                    "Type": "mouthDown",
                    "X": 0.4837512969970703,
                    "Y": 0.6288153529167175
                },
                {
                    "Type": "leftPupil",
                    "X": 0.4091112017631531,
                    "Y": 0.3714003264904022
                },
                {
                    "Type": "rightPupil",
                    "X": 0.5479598641395569,
                    "Y": 0.36694735288619995
                },
                {
                    "Type": "upperJawlineLeft",
                    "X": 0.32060497999191284,
                    "Y": 0.37316644191741943
                },
                {
                    "Type": "midJawlineLeft",
                    "X": 0.354483425617218,
                    "Y": 0.6085302233695984
                },
                {
                    "Type": "chinBottom",
                    "X": 0.48467764258384705,
                    "Y": 0.74000084400177
                },
                {
                    "Type": "midJawlineRight",
                    "X": 0.6017045378684998,
                    "Y": 0.5998770594596863
                },
                {
                    "Type": "upperJawlineRight",
                    "X": 0.6245695352554321,
                    "Y": 0.36280637979507446
                }
            ],
            "Pose": {
                "Roll": -1.2611076831817627,
                "Yaw": 1.3410354852676392,
                "Pitch": 3.3579587936401367
            },
            "Quality": {
                "Brightness": 87.4839096069336,
                "Sharpness": 89.85481262207031
            },
            "Confidence": 99.99876403808594,
            "FaceOccluded": {
                "Value": false,
                "Confidence": 99.87444305419922
            },
            "EyeDirection": {
                "Yaw": 2.2627758979797363,
                "Pitch": -2.8187551498413086,
                "Confidence": 99.9777603149414
            }
        }
    ],
    "ResponseMetadata": {
        "RequestId": "7e60fd67-407f-45b0-aad3-2447edea4120",
        "HTTPStatusCode": 200,
        "HTTPHeaders": {
            "x-amzn-requestid": "7e60fd67-407f-45b0-aad3-2447edea4120",
            "content-type": "application/x-amz-json-1.1",
            "content-length": "3432",
            "date": "Tue, 09 Jul 2024 10:58:17 GMT"
        },
        "RetryAttempts": 0
    }
}
```
### Step 4: Extract Value/Confidence for Smile, Sunglasses, EyesOpen, MouthOpen, CalmEmotion

```py
import json
import boto3
import uuid
import datetime

BUCKET_NAME = "cloudtopia-images-jun-2024" 
FACE_DETAILS_THRESHOLDS = {"Smile": {"desiredValue": False, "minConfidence": 90}, "Sunglasses": {"desiredValue": False, "minConfidence": 90}, "EyesOpen": {"desiredValue": True, "minConfidence": 90}, "MouthOpen": {"desiredValue": False, "minConfidence": 90}}

rekognition_client = boto3.client('rekognition')

def lambda_handler(event, context):
    #Step 1 - Extract File Name from PUT Event
    current_file_name = extract_file_name(event)
    
    #Step 2 - Call Rekognition DetectFaces API
    detect_faces_response = detect_faces(current_file_name)
    print(json.dumps(detect_faces_response))
    
    #Step 3 - Extract face details we care about and their value/confidence
    face_details = extract_face_details(detect_faces_response) #extract the attributes we care about
    print(face_details)
    
def extract_face_details(result):
    parsed_response = {} 
    
    face_details = result["FaceDetails"]
    face = face_details[0]
    
    #iterate over all fields we care about and extract the details
    for key, value in FACE_DETAILS_THRESHOLDS.items():
        parsed_response[key] = face[key]
        
    return parsed_response
    
def detect_faces(file_name):
    return rekognition_client.detect_faces(
    Image={
        'S3Object': {
            'Bucket': BUCKET_NAME,
            'Name': file_name
        }
    },
    Attributes=['ALL']
    )

def extract_file_name(event):
    return event["Records"][0]["s3"]["object"]["key"]
```
* Click `Deploy`
* Click `Test`

### Step 5: Apply Pass/Fail Business Logic (Confidence => 90.00 && Value)

```py
import json
import boto3
import uuid
import datetime

BUCKET_NAME = "cloudtopia-images-jun-2024" 
FACE_DETAILS_THRESHOLDS = {"Smile": {"desiredValue": False, "minConfidence": 90}, "Sunglasses": {"desiredValue": False, "minConfidence": 90}, "EyesOpen": {"desiredValue": True, "minConfidence": 90}, "MouthOpen": {"desiredValue": False, "minConfidence": 90}}

rekognition_client = boto3.client('rekognition')

def lambda_handler(event, context):
    #Step 1 - Extract File Name from PUT Event
    current_file_name = extract_file_name(event)
    
    #Step 2 - Call Rekognition DetectFaces API
    detect_faces_response = detect_faces(current_file_name)
    
    #Step 3 - Extract face details we care about and their value/confidence
    face_details = extract_face_details(detect_faces_response) #extract the attributes we care about
    
    #Step 4 - Evaluates values and thresholds to determine PASS/FAIL
    face_evaluation_result = evaluate_face(face_details)
    print(face_evaluation_result)
    
def evaluate_face(parsed_face_scan):

    evaluation_result = {
        "result": "PASS", #we assume it is a pass unless we prove otherwise
        "failure_reasons": []
    }
    
    for key, value in FACE_DETAILS_THRESHOLDS.items():
        temp_result = True #assume we pass the test
        if parsed_face_scan[key]["Value"] != FACE_DETAILS_THRESHOLDS[key]["desiredValue"]:
            print(f"Expected != Actual. FaceAttribute: {key} has a value: {parsed_face_scan[key]['Value']}, but requires {FACE_DETAILS_THRESHOLDS[key]['desiredValue']}")
            temp_result = False
        if parsed_face_scan[key]["Confidence"] <= FACE_DETAILS_THRESHOLDS[key]["minConfidence"]:
            print(f"Confidence is lower than minimum threshold. FaceAttribute: {key} has a confidence value: {parsed_face_scan[key]['Confidence']}, but must be greater than {FACE_DETAILS_THRESHOLDS[key]['minConfidence']}")
            temp_result = False
        
        if temp_result is False:
            evaluation_result["result"] = "FAIL"
            evaluation_result["failure_reasons"].append(key)
    return evaluation_result

    
def extract_face_details(result):
    parsed_response = {} 
    
    face_details = result["FaceDetails"]
    face = face_details[0]
    
    #iterate over all fields we care about and extract the details
    for key, value in FACE_DETAILS_THRESHOLDS.items():
        parsed_response[key] = face[key]
        
    return parsed_response
    
def detect_faces(file_name):
    return rekognition_client.detect_faces(
    Image={
        'S3Object': {
            'Bucket': BUCKET_NAME,
            'Name': file_name
        }
    },
    Attributes=['ALL']
    )

def extract_file_name(event):
    return event["Records"][0]["s3"]["object"]["key"]
```
* Click `Deploy`
* Click `Test`

To test the photos further, upload 2 variant photos:

![me_smiling](/assets/me_smiling.png)

* Check `Cloudwatch logs` for me_smiling.png:

```json
{'result': 'FAIL', 'failure_reasons': ['Smile', 'MouthOpen']}

Expected != Actual. FaceAttribute: MouthOpen has a value: True, but requires False

Expected != Actual. FaceAttribute: Smile has a value: True, but requires False
```

* Check `Cloudwatch logs` for me_eyes_closed.png:

![me_eyes_closed.png](/assets/me_eyes_closed.pmg)

```json
{'result': 'FAIL', 'failure_reasons': ['EyesOpen', 'MouthOpen']}

Confidence is lower than minimum threshold. FaceAttribute: MouthOpen has a confidence value: 89.87174987792969, but must be greater than 90

Expected != Actual. FaceAttribute: EyesOpen has a value: False, but requires True
```







